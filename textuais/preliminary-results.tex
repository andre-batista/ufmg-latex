% ------------------------------------------------------------------------------
% Preliminary Results
% ------------------------------------------------------------------------------

\chapter{Preliminary Results}\label{chap:results}

	In Section \ref{chap:investigation:proposal}, three investigations were proposed and some ideas were already implemented. Since the investigation presented in Subsection \ref{chap:investigation:proposal:performance} proposes a benchamarking plataform, an example of its use was left to Appendix \ref{annex:bimreview} since the example addresses a question which was not announced in the last chapter. Therefore, this chapter aims to present the preliminary results of investigations proposed in Subsections \ref{chap:investigation:proposal:stateofart} and \ref{chap:investigation:proposal:operators}.
	
	We will start with the evaluation of the L-SHADE algorithm in Section \ref{chap:results:lshade}. Three different situations are explored: low resolution (Subsection \ref{chap:results:lshade:low}), higher resolution (Subsection \ref{chap:results:lshade:higher}), and IMSA integration (Subsection \ref{chap:results:lshade:imsa}). Then, CA will be explored in Section \ref{chap:results:ca}. Firstly, some results will demonstrate some properties (Subsection \ref{chap:results:ca:demo}). Then, the application as initialization method is explored in Subsection \ref{chap:results:ca:application}. The experiments are still not able to answer the questions raised in the last chapter. However, some considerations are presented in Section \ref{chap:results:conclusion} with assessments and recommendations based on the information obtained through the experiments.

	\section{Evaluating L-SHADE Algorithm}\label{chap:results:lshade}

		As stated in \autoref{chap:investigation:proposal:stateofart}, the research proposes to investigate the application of state-of-art algorithms to EISP. The L-SHADE algorithm is the first implementation that we will consider. Initially, we want to evaluated its performance in very simple scenario: a centered circle. Therefore, it is a case-study in which we want to compare the performance among L-SHADE, SHADE (removal of dynamic population size strategy), DE, GA, and PSO. Their operational parameters are shown in \autoref{tab:results:algparameters}. While the parameters in PSO were set to the traditional values \citep{salucci2017multifrequency}, the others were set without a rigorous evaluation yet. However, they are still similar to those employed in the EC literature \cite{eiben2015introduction}.
		
		\begin{table}
			\centering
			\def\arraystretch{1.5}
			\caption{Operational parameters for the considered EAs.}
			\begin{tabular}{clc}
				\textbf{Algorithms}                       & \textbf{Parameters}                             & \textbf{Value} \\\hline\hline
				\multirow{2}{*}{PSO}           			 & Inertia Weight                         			  & 0.4   \\
																	 & Acceleration Coefficients              		& 2.0   \\\hline
				DE                              		 		   & Mutation Faction ($F$)                   	  & 0.5   \\\hline
				\multirow{4}{*}{GA}             		  & Crossover probability                  		   & 80\%  \\
																	 & Mutation probability                   		   & 20\%  \\
																	 & SBX Factor                             			   & 0.1   \\
																	 & PBM Factor                             			   & 50    \\\hline
				\multirow{2}{*}{L-SHADE, SHADE} & Archive Size                           			  & 20    \\
																	  & Initial Crossover and Mutation Factors & 0.5  
			\end{tabular}
			\label{tab:results:algparameters}
		\end{table}
		
		\subsection{Low-Resolution}\label{chap:results:lshade:low}

			First of all, a low number of variables will be considered. The recovered images were set to $5\times5$ elements. Therefore, $25+N_S25$ variables were solved in three different cases: weak, medium, and strong scatterer. The common parameters among the cases are described in \autoref{tab:results:lshade:low}. It is worth noting that the population size ($N_{POP}$) is defined according to \citep{salucci2017multifrequency}. The maximum contrast ($\chi_{max}$) was defined slight above the true one. While it represents a slight deviation from the \textit{a priori} knowledge of true contrast, it also reduces the chance of being trapped in a false solution, i.e., a local minima which does not represent a reasonable image.
			
			\begin{table}
				\centering
				\def\arraystretch{1.5}
				\caption{General parameters for low-resolution L-SHADE evaluation study.}
				\begin{tabular}{ccccccc}
					$\lambda_b$ & $L_X$, $L_Y$ & $\chi$ & $R_O$ & $N_{POP}$ & $\chi_{min}$ & $\chi_{max}$ \\\hline
					 1 [m] & 2$\lambda_b$ & 1 & 1.1$\sqrt{2}L_X$ & 250 & 0 & 2
				\end{tabular}
				\label{tab:results:lshade:low}
			\end{table}
		
			The weak scatterer scenario is addressed defining the circle radius as $0.2\lambda_b$. The value is based on the fitted curve for contrast-size relation for BIM as discussed in Appendix \ref{annex:contrastsize}. Therefore, this choice is below the curve. Each algorithm was run 30 times and the algorithms stopped after 600,000 evaluations (this value is below than the used in \citep{salucci2017multifrequency}). The scattered field was synthesized through the analytical without adding noise. The number of sources and measurements followed the degrees of freedom of the problem.
			
			\autoref{fig:results:lshade:low:weak:images} shows the best recovered contrast image considering the lowest objective-function evaluation of the final solution among the 30 executions. The object contrast was better estimated by DE, SHADE, and L-SHADE, while the cleanest background area was obtained by PSO. GA does not seem to be able to recover good images. The difficulty in retrieving a higher contrast value in object is possibly due to low-resolution condition.
			
			\begin{figure}
				\centering
				\subfloat[Instance]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_weak_inst}}
				\subfloat[PSO]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_weak_pso}}
				\subfloat[GA]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_weak_ga}} \\
				\subfloat[DE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_weak_de}}
				\subfloat[SHADE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_weak_shade}}
				\subfloat[L-SHADE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_weak_lshade}}
				\caption[L-SHADE evaluation, low-resolution, weak scatterer: best recovered images.]{L-SHADE evaluation, low-resolution, weak scatterer: best images recovered by each algorithm among 30 executions according to minimum objective function evaluation criterion.}
				\label{fig:results:lshade:low:weak:images}
			\end{figure}
		
			\autoref{fig:results:lshade:low:weak:convergence} shows the convergence of each algorithm, i.e., the evolution of the best solution in each generation throughout the execution. SHADE and L-SHADE reached the smallest values while DE converged faster to a reasonable level. GA and PSO seem to be too slow compared with the others. It is worth noting the frequency in which the algorithms reach the same level at the end of the generations, especially DE. 
		
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth]{./figuras/results/lshade_low_weak_conv}
				\caption[L-SHADE evaluation, low-resolution, weak scatterer: convergence.]{L-SHADE evaluation, low-resolution, weak scatterer: convergence of objective function by each algorithm considering 30 executions.}
				\label{fig:results:lshade:low:weak:convergence}
			\end{figure}
			
			\autoref{fig:results:lshade:low:weak:indicators} shows the $\zeta_{\epsilon PAD}$ \eqref{eq:4:zeta:epad}, $\zeta_{\epsilon OE}$ \eqref{eq:4:zeta:eoe}, $\zeta_{TFMPAD}$ \eqref{eq:4:zeta:tfmpad}, and $\zeta_{TFPAD}$ \eqref{eq:4:zeta:tfpad} values measured according to the final solutions of each execution. While PSO seems to be better in estimating the contrast value considering the whole image, DE, SHADE, and L-SHADE seem to be better in estimating the scatterer contrast. Looking to the electric field, DE, SHADE, and L-SHADE were better than PSO and GA in all executions. Regarding the phase estimation error, DE had a smaller error than the others. When we compare DE, SHADE, and L-SHADE considering the $\zeta_{\epsilon OE}$ indicator, evidence is found against means equality\footnote{The residual distribution was far from normal. Then Kruskal-Wallis test was used} (p-value: $1.3206\times10^{-8}$). When we compare individual means, we have find superiority of SHADE over L-SHADE and DE (p-value=$4.2424\times10^{-9}$ and p-value=$1.91747\times10^{-6}$, respectively). When we compare SHADE' and L-SHADE's\footnote{Although it is not possible to see in Figure \ref{fig:results:lshade:low:weak:indicators:tfmpad}, all DE's quartiles are above SHADE and L-SHADE ones.} $\zeta_{TFMPAD}$ indicators, there is statistical evidence for superiority of later (Mann-Whitney test p-value: $1.7371\times10^{-10}$).

			\begin{figure}
				\centering
				\subfloat[$\zeta_{\epsilon PAD}$]{\includegraphics[width=.5\textwidth]{./figuras/results/lshade_low_weak_zeta_epad}}
				\subfloat[$\zeta_{\epsilon OE}$]{\includegraphics[width=.5\textwidth]{./figuras/results/lshade_low_weak_zeta_eoe}} \\
				\subfloat[$\zeta_{TFMPAD}$]{\includegraphics[width=.5\textwidth]{./figuras/results/lshade_low_weak_zeta_tfmpad}\label{fig:results:lshade:low:weak:indicators:tfmpad}}
				\subfloat[$\zeta_{TFPAD}$]{\includegraphics[width=.5\textwidth]{./figuras/results/lshade_low_weak_zeta_tfppad}}
				\caption[L-SHADE evaluation, low-resolution, medium scatterer: indicators.]{L-SHADE evaluation, low-resolution, medium scatterer: quartiles of performance indicators.}
				\label{fig:results:lshade:low:weak:indicators}
			\end{figure}
					
			Regarding the medium scatterer case, the radius was set to $0.5\lambda_b$, which is close to the curve in \ref{fig:annex:contrastsize:size}. The number of sources and measurements were updated and the number of evaluations was increased to one million. The data synthesis followed the same process as the weak scatterer study.
			
			\autoref{fig:results:lshade:low:medium:images} shows the best recovered images according to the minimum objective-function evaluation criterion. The same observations made for the weak scatterer can be made again: PSO had the cleanest background area and DE, SHADE, and L-SHADE had the most similar shapes comparing against the instance. However, it is worth noting that the scatterer obtained by PSO was not at the center of the image as we would expect.
			
			\begin{figure}
				\centering
				\subfloat[Instance]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_medium_inst}}
				\subfloat[PSO]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_medium_pso}}
				\subfloat[GA]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_medium_ga}} \\
				\subfloat[DE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_medium_de}}
				\subfloat[SHADE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_medium_shade}}
				\subfloat[L-SHADE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_medium_lshade}}
				\caption[L-SHADE evaluation, low-resolution, medium scatterer: best recovered images.]{L-SHADE evaluation, low-resolution, medium scatterer: best images recovered by each algorithm among 30 executions according to minimum objective function evaluation criterion.}
				\label{fig:results:lshade:low:medium:images}
			\end{figure}
		
			\autoref{fig:results:lshade:low:medium:convergence} shows the convergence. Again, the same observations can be made: (i) GA and PSO had a slower convergence than the other algorithms; and (ii) DE converges so much more faster than SHADE and L-SHADE; (iii) the final objective-function evaluations among all executions were very similar (standard deviation $\approx0.0001$). Comparing SHADE against L-SHADE, SHADE has a smoother convergence than L-SHADE.
		
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth]{./figuras/results/lshade_low_medium_conv}
				\caption[L-SHADE evaluation, low-resolution, medium scatterer: convergence.]{L-SHADE evaluation, low-resolution, medium scatterer: convergence of objective function by each algorithm considering 30 executions.}
				\label{fig:results:lshade:low:medium:convergence}
			\end{figure}
		
			\autoref{fig:results:lshade:low:medium:indicators} shows the $\zeta_{\epsilon PAD}$, $\zeta_{\epsilon OE}$, $\zeta_{TFMPAD}$, and $\zeta_{TFPAD}$ values, as well as objective-function evaluation, measured according the final solutions of each execution. The results are similar to ones obtained for weak scattering. There are statistical evidences to support the better performance in average of SHADE against L-SHADE considering each indicator and objective function, except $\zeta_{\epsilon PAD}$.
		
			\begin{figure}
				\centering
				\subfloat[$\zeta_{\epsilon PAD}$]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_low_medium_zeta_epad}}
				\subfloat[$\zeta_{\epsilon OE}$]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_low_medium_zeta_eoe}} 
				\subfloat[$\zeta_{TFMPAD}$]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_low_medium_zeta_tfmpad}} \\
				\subfloat[$\zeta_{TFPAD}$]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_low_medium_zeta_tfppad}}
				\subfloat[Objective Function]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_low_medium_objfun}}
				\caption[L-SHADE evaluation, low-resolution, medium scatterer: indicators.]{L-SHADE evaluation, low-resolution, medium scatterer: quartiles of performance indicators.}
				\label{fig:results:lshade:low:medium:indicators}
			\end{figure}
		
			Regarding the strong scatterer study, the radius ($a$) has been increased to $0.8\lambda_b$, which is above the fitted curve in \autoref{fig:annex:contrastsize:size}. Consequently, the image size has been changed to $4a\times4a$. A million of evaluations is still considered as well as the other parameters in weak scattering.
			
			\autoref{fig:results:lshade:low:strong:images} shows the best recovered images by each algorithm according to the objective function evaluation. None of the algorithms was able to recover something similar to what was obtained in the medium scattering. Analyzing the convergence in \autoref{fig:results:lshade:low:strong:convergence}, while PSO was again too slow to converge, the others did converged or finished the execution with small variations in the objective function. Although none of the algorithms have reached a lower level than DE, it also did not reach a level below 1. The indicators will not be analyzed since the images were not reasonable.
		
			\begin{figure}
				\centering
				\subfloat[Instance]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_strong_inst}}
				\subfloat[PSO]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_strong_pso}}
				\subfloat[GA]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_strong_ga}} \\
				\subfloat[DE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_strong_de}}
				\subfloat[SHADE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_strong_shade}}
				\subfloat[L-SHADE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_low_strong_lshade}}
				\caption[L-SHADE evaluation, low-resolution, strong scatterer: best recovered images.]{L-SHADE evaluation, low-resolution, strong scatterer: best images recovered by each algorithm among 30 executions according to minimum objective function evaluation criterion.}
				\label{fig:results:lshade:low:strong:images}
			\end{figure}
		
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth]{./figuras/results/lshade_low_strong_conv}
				\caption[L-SHADE evaluation, low-resolution, strong scatterer: convergence.]{L-SHADE evaluation, low-resolution, strong scatterer: convergence of objective function by each algorithm considering 30 executions.}
				\label{fig:results:lshade:low:strong:convergence}
			\end{figure}
		
		\subsection{Higher Resolution}\label{chap:results:lshade:higher}
		
			Now, moving to a higher number of variables, we want to evaluate the performance of the algorithms. The image resolution was increased to $10\times10$ elements, which equals $100 + 100N_S$ variables. For this study, we have considered only weak scattering. The circle contrast was set to $0.5$, and its radius, $0.4\lambda_b$, which is considerably below the curve in \autoref{fig:annex:contrastsize:size}. The scattered field is synthesized as in the past experiments. The same parameters in \autoref{tab:results:lshade:low} are still considered, except $L_X$ and $L_Y$ which have been set to $4a$ both. A million of evaluations in each of 30 executions was considered.
			
			\autoref{fig:results:lshade:higher:images} shows the best images recovered by each algorithm according to best objective function evaluation criterion. SHADE and L-SHADE recovered similar patterns. Their contrast estimation was reasonable and the images look like a centered circle. The same was not true for the other algorithms. Although DE seemed to detect a scatterer at the center, its shape was considerably irregular.
			
			\begin{figure}
				\centering
				\subfloat[Instance]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_higher_inst}}
				\subfloat[PSO]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_higher_pso}}
				\subfloat[GA]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_higher_ga}} \\
				\subfloat[DE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_higher_de}}
				\subfloat[SHADE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_higher_shade}}
				\subfloat[L-SHADE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_higher_lshade}}
				\caption[L-SHADE evaluation, higher resolution: best recovered images.]{L-SHADE evaluation, higher resolution: best images recovered by each algorithm among 30 executions according to minimum objective function evaluation criterion.}
				\label{fig:results:lshade:higher:images}
			\end{figure}
		
			Looking at the convergence of objective function in \autoref{fig:results:lshade:higher:convergence}, DE has reached the lowest levels. However, it did not necessarily imply in a good image. In fact, when we look at performance indicators in \autoref{fig:results:lshade:higher:indicators}, DE did not have the lowest $\zeta_{\epsilon PAD}$ and $\zeta_{\epsilon OE}$ errors. On the other hand, $\zeta_{TFMPAD}$ and $\zeta_{TFPAD}$ were constantly low comparing against the other algorithms. Therefore, it seems that the good estimation of the electric field has more impact on the objective function than the contrast one, probably due to its large number of variables.
		
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth]{./figuras/results/lshade_higher_conv}
				\caption[L-SHADE evaluation, higher resolution: convergence.]{L-SHADE evaluation, higher resolution: convergence of objective function by each algorithm considering 30 executions.}
				\label{fig:results:lshade:higher:convergence}
			\end{figure}
		
			We also have applied statistical tests to compare SHADE against L-SHADE considering each indicator plus the final objective function value. For a 5\% significance level and all assured assumptions, there was no evidence for difference in average performance for all cases, i.e., the equality of means hypothesis was not rejected in any case. Therefore, considering the average case, both algorithms tend to have similar results in this case study. 
		
			\begin{figure}
				\centering
				\subfloat[$\zeta_{\epsilon PAD}$]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_low_medium_zeta_epad}}
				\subfloat[$\zeta_{\epsilon OE}$]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_low_medium_zeta_eoe}} 
				\subfloat[$\zeta_{TFMPAD}$]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_low_medium_zeta_tfmpad}} \\
				\subfloat[$\zeta_{TFPAD}$]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_low_medium_zeta_tfppad}}
				\subfloat[Objective Function]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_low_medium_objfun}}
				\caption[L-SHADE evaluation, higher resolution: indicators.]{L-SHADE evaluation, higher resolution: quartiles of performance indicators.}
				\label{fig:results:lshade:higher:indicators}
			\end{figure}
		
		\subsection{Coupling the IMSA strategy to the algorithm}\label{chap:results:lshade:imsa}
		
			Solving the model in multiple resolution is an important strategy in the literature. Therefore, L-SHADE must be evaluated also when coupled with IMSA. If L-SHADE can converge faster than PSO, then its application can improve the state-of-art of EAs applied to EISP.
			
			First of all, we will analyze the performance of the algorithm when the clustering process is not applied. We want to evaluate the multi-resolution strategy without eliminating the regions of the image. The case study will consider again the centered circle with the following properties: $\chi=1$ and $a=0.25\lambda_b$. This is below the curve in Figure \ref{fig:annex:contrastsize:size}, i.e., a weak scatterer. The case study will also consider the three algorithms: PSO, DE, and L-SHADE. Three resolutions will also be considered in the following order: $5\times5$, $10\times10$, and $15\times15$. Each resolution step will have 20,000 evaluations and the same population size will be considered. Therefore, the algorithms will stop very earlier than the last experiments, where the goal was to observe the best that each algorithm can reach. Here, we will evaluate the performance with a shorter time. The upper bound for the contrast variables will be set to the true value. Hence, the search space is smaller than before. The scattered field data were synthesized according to the degree of freedom of the problem.
			
			\autoref{fig:results:lshade:imsa:noclus:images} shows the best images recovered by each algorithm according to the objective function evaluation. Only IMSA-L-SHADE was able to recover a figure similar to the circle. The contrast range within the image was also reasonable.
			
			\begin{figure}
				\centering
				\subfloat[Instance]{\includegraphics[width=0.5\textwidth]{./figuras/results/lshade_imsa_noclus_inst}}
				\subfloat[IMSA-PSO]{\includegraphics[width=0.5\textwidth]{./figuras/results/lshade_imsa_noclus_pso}} \\
				\subfloat[IMSA-DE]{\includegraphics[width=0.5\textwidth]{./figuras/results/lshade_imsa_noclus_de}}
				\subfloat[IMSA-L-SHADE]{\includegraphics[width=0.5\textwidth]{./figuras/results/lshade_imsa_noclus_lshade}}
				\caption[IMSA-L-SHADE evaluation, no clustering: best recovered images.]{IMSA-L-SHADE evaluation, no clustering: best images recovered by each algorithm among 30 executions according to minimum objective function evaluation criterion.}
				\label{fig:results:lshade:imsa:noclus:images}
			\end{figure}
		
			\autoref{fig:results:lshade:imsa:noclus:convergence} shows the convergence of each algorithm. While DE used to converge faster than L-SHADE in the last experiments, here IMSA-L-SHADE was the fastest and it also reached the lowest values in all executions. The transition between resolutions is very clear on the IMSA-L-SHADE curve. When the resolution is increased, then the solutions are expected to have a worse objective function evaluation since the integration embedded in the computation will be more accurate.
							
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth]{./figuras/results/lshade_imsa_noclus_conv}
				\caption[IMSA-L-SHADE evaluation, no clustering: convergence.]{IMSA-L-SHADE evaluation, no clustering: convergence of objective function by each algorithm considering 30 executions.}
				\label{fig:results:lshade:imsa:noclus:convergence}
			\end{figure}
		
			\autoref{fig:results:lshade:imsa:noclus:indicators} shows some of the performance indicators. The images recovered by PSO used to have low contrast values everywhere in the image. This can explain why some of its $\zeta_{\epsilon PAD}$ values were as low or even smaller then IMSA-L-SHADE even though images did not show a clear scatterer. But, when the $\zeta_{\epsilon OE}$ indicator is considered. Then a clear advantage of IMSA-L-SHADE against IMSA-PSO is seen. Comparing IMSA-L-SHADE against IMSA-PSO through the same indicator, there are evidences in favor of the first for its best average performance (T-Test's p-value $<10^{-14}$). Now, looking at the $\zeta_{TFMPAD}$, even though IMSA-L-SHADE seems to have the best average performance, all algorithms had high error levels.
		
			\begin{figure}
				\centering
				\subfloat[$\zeta_{\epsilon PAD}$]{\includegraphics[width=.5\textwidth]{./figuras/results/lshade_imsa_noclus_epad}}
				\subfloat[$\zeta_{\epsilon OE}$]{\includegraphics[width=.5\textwidth]{./figuras/results/lshade_imsa_noclus_eoe}} \\
				\subfloat[$\zeta_{TFMPAD}$]{\includegraphics[width=.5\textwidth]{./figuras/results/lshade_imsa_noclus_tfmpad}}
				\subfloat[$\zeta_{TFPAD}$]{\includegraphics[width=.5\textwidth]{./figuras/results/lshade_imsa_noclus_tfppad}}
				\caption[IMSA-L-SHADE evaluation, no clustering: indicators.]{IMSA-L-SHADE evaluation, no clustering: quartiles of performance indicators.}
				\label{fig:results:lshade:imsa:noclus:indicators}
			\end{figure}
		
			The same problem was also considered to analyze the impact of the clustering process on each algorithm. The best images according to the objective function evaluation are shown in \autoref{fig:results:lshade:imsa:clus:images}. The best image recovered by IMSA-L-SHADE does not look like a circle but it did detected a scatterer at the center. The best images recovered by DE and PSO are small scatterers apart from the middle. In these cases the clustering process eliminated a significant portion of the image and the algorithm tended to increase the contrast in the remaining region. 
		
			\begin{figure}
				\centering
				\subfloat[IMSA-PSO]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_imsa_clus_pso}}
				\subfloat[IMSA-DE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_imsa_clus_de}}
				\subfloat[L-SHADE]{\includegraphics[width=0.3\textwidth]{./figuras/results/lshade_imsa_clus_lshade}}
				\caption[IMSA-L-SHADE evaluation, w/ clustering: best recovered images.]{IMSA-L-SHADE evaluation, w/ clustering: best images recovered by each algorithm among 30 executions according to minimum objective function evaluation criterion.}
				\label{fig:results:lshade:imsa:clus:images}
			\end{figure}
		
			The convergence is shown in \autoref{fig:results:lshade:imsa:clus:convergence}. Differently from \autoref{fig:results:lshade:imsa:noclus:convergence}, there are cases in which the transition between resolutions represent a decay in the objective function. This is likely to happen when the objective function is too high. In these cases, the finer integration reduces the error even when the solution is still far from the optimum. In addition, IMSA-DE has reached the same level than IMSA-L-SHADE.
		
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth]{./figuras/results/lshade_imsa_clus_conv}
				\caption[IMSA-L-SHADE evaluation, w/clustering: convergence.]{IMSA-L-SHADE evaluation, w/ clustering: convergence of objective function by each algorithm considering 30 executions.}
				\label{fig:results:lshade:imsa:clus:convergence}
			\end{figure}
		
			The $\zeta_{\epsilon PAD}$ and $\zeta_{\epsilon OE}$ indicators as well as the objective function evaluation of final solutions are shown in \autoref{fig:results:lshade:imsa:clus:indicators}. The $\zeta_{TFMPAD}$ and $\zeta_{TFPAD}$ are excluded since the electric field is not considered in removed areas. Even though IMSA-L-SHADE and IMSA-DE had similar performance considering the objective function evaluation, statistical tests indicated evidences for a larger likelihood of observing a better $\zeta_{\epsilon PAD}$ and $\zeta_{\epsilon OE}$ performances by IMSA-L-SHADE (p-values of Mann-Whitney rank test $<10^{-4}$).
			
			\begin{figure}
				\centering
				\subfloat[$\zeta_{\epsilon PAD}$]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_imsa_clus_epad}}
				\subfloat[$\zeta_{\epsilon OE}$]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_imsa_clus_eoe}}
				\subfloat[Objective Function]{\includegraphics[width=.3\textwidth]{./figuras/results/lshade_imsa_clus_fx}}
				\caption[IMSA-L-SHADE evaluation, w/ clustering: indicators.]{IMSA-L-SHADE evaluation, w/ clustering: quartiles of performance indicators.}
				\label{fig:results:lshade:imsa:clus:indicators}
			\end{figure}
		
			
	\section{Evaluating Circle Approximation}\label{chap:results:ca}
	
		As stated in \autoref{chap:investigation:proposal:operators}, the research also proposes to investigate specialized evolutionary operators to the problem. Initially, an approach to address strong scatterers was developed. The Circle Approximation (CA) is based on the analytical scattering of a circular cylinder, i.e., the method aims to find the radius, position, and contrast of the circle which best fits into the given data. The method determines the three parameters through an implementation of GA with multiples islands.
		
		\subsection{Algorithm Demonstration}\label{chap:results:ca:demo}
		
			Many experiments were carried out to validate the approximation. We want to highlight one of them to demonstrate some properties. A shifted triangle was considered and its contrast and side length were set 5 and 0.35$\lambda_b$, respectively. This configuration is considerably above the curve in Figure \ref{fig:annex:contrastsize:size}. We have divided the population into 5 islands where each of them addressed a portion of the contrast range, defined as $[0, 50]$. Therefore, the range is divided into 5 equally spaced portions and the algorithm is tested in a condition which is similar to the absence of \textit{a priori} knowledge of the true contrast. No random noise was added to the scattered field data in order to avoid the impact of this factor in our prior analysis. A population of 100 individuals was divided into the islands, i.e., 20 individuals per island. The algorithm stopped after 3,000 evaluations.
			
			\autoref{fig:results:ca:demo:radiusposition} shows the best circles found by each island in each execution according to the best objective function evaluation criterion. The position of the scatterer seems to be an easy information to recover since all circles seem to be centered at the same position as the triangle is, approximately. However, the radius seems to be a harder aspect since a smaller circle was obtained in some cases, except in the first island where $\chi\in[0,10]$ and it includes the true value (Figure \ref{fig:results:ca:demo:radiusposition:1}).
			
			\begin{figure}
				\centering
				\subfloat[Instance]{\includegraphics[width=.3\textwidth]{./figuras/results/ca_demo_pr_inst.eps}}
				\subfloat[{$\chi\in[0,10]$}]{\includegraphics[width=.3\textwidth]{./figuras/results/ca_demo_pr_isl0.eps}\label{fig:results:ca:demo:radiusposition:1}}
				\subfloat[{$\chi\in[10,20]$}]{\includegraphics[width=.3\textwidth]{./figuras/results/ca_demo_pr_isl1.eps}} \\
				\subfloat[{$\chi\in[20,30]$}]{\includegraphics[width=.3\textwidth]{./figuras/results/ca_demo_pr_isl2.eps}}
				\subfloat[{$\chi\in[30,40]$}]{\includegraphics[width=.3\textwidth]{./figuras/results/ca_demo_pr_isl3.eps}}
				\subfloat[{$\chi\in[40,50]$}]{\includegraphics[width=.3\textwidth]{./figuras/results/ca_demo_pr_isl4.eps}} \\
				\caption[CA's demonstration study: circles estimated by each island in the demonstration study.]{Circles estimated by each island in the demonstration study.}
				\label{fig:results:ca:demo:radiusposition}
			\end{figure}
		
			\autoref{fig:results:ca:demo:radiusposition:box} shows the error in position and radius estimation. The position error was evaluated by the radius between the triangle and a circle, i.e., their centers. The radius error was evaluated considering the minimum circle that fits the triangle as reference. No island has a clear superiority in both metrics. In fact, when we applied Kruskal-Wallis test\footnote{Since the normality and homoscedasticity assumptions were not satisfied, a non-parametric test was employed.}, it fails in rejecting the equality of medians hypotheses at 5\% level of significance in both metrics.
			
			\begin{figure}
				\centering
				\subfloat[Position]{\includegraphics[width=.5\textwidth]{./figuras/results/ca_demo_pr_pbox}}
				\subfloat[Radius]{\includegraphics[width=.5\textwidth]{./figuras/results/ca_demo_pr_rbox}}
				\caption[CA's demonstration study: position and radius errors quartiles.]{Quartiles of position and radius errors of each final solution in each island.}
				\label{fig:results:ca:demo:radiusposition:box}
			\end{figure}
			
			\autoref{fig:results:ca:demo:x} shows the contrast values of final solutions found by the first island which contains the true value. Although the median and mean values are close to the ground truth (3.87 and 4.07, respectively), the histogram shows that the range $[4.5,5.5]$ was only the fourth most obtained among the executions. This might be related to the error of approximating a triangle to a circle. A narrower range could improve the number of times that the island converges to the true value.
			
			\begin{figure}
				\centering
				\subfloat[]{\includegraphics[width=.5\textwidth]{./figuras/results/ca_demo_x_box}}
				\subfloat[]{\includegraphics[width=.5\textwidth]{./figuras/results/ca_demo_x_hist}}
				\caption[CA's demonstration study: contrast values of final solutions found by the first island]{Contrast values of final solutions found by the first island.}
				\label{fig:results:ca:demo:x}
			\end{figure}
		
			\autoref{fig:results:ca:demo:fx} shows the quartiles of the best objective function evaluation found by each island. The overall best objective function evaluation is achieved by the first island and its contrast value is very close to the true one ($\approx 4.9$). None of the boxes seems to vary from each other. There are many intersections among them, which means that other islands can reach values as low as the one which contains the true contrast. In fact, \autoref{tab:results:ca:demo:fx} shows that, in 11 executions (37\%), other islands rather than the first obtained the best objective function evaluation. However, if we apply Mann-Whitney rank tests\footnote{Considerable deviations from normal distribution were detected. Therefore, a non-parametric test was used.} to compare the first island against each others, there are evidences for a greater probability of observing a lowest objective function when the first island is considered at 5\% significance level. When we run Kruskal-Wallis test considering the third, fourth, and fifth islands, it fails in rejecting the hypothesis of equal median performance.
			
			\begin{figure}
				\centering
				\includegraphics[width=.5\textwidth]{./figuras/results/ca_demo_fx}
				\caption[CA's demonstration study: objective function evaluation of final solutions found by each island.]{Objective function evaluation of final solutions found by each island.}
				\label{fig:results:ca:demo:fx}
			\end{figure}
		
			\begin{table}
				\centering
				\def\arraystretch{1.5}
				\caption[CA's demonstration study: number of wins of each island.]{Number of times that each island reached the minimum objective function evaluation at the end of the execution (wins).}
				\begin{tabular}{cccccc}
					Island & [0,10] & [10,20] & [20,30] & [30,40] & [40,50] \\\hline
					Wins & 18 & 0 & 4 & 7 & 1
				\end{tabular}
				\label{tab:results:ca:demo:fx}
			\end{table}
		
			Finally, CA can detect reasonably shape and position of single scatterers, no matter its contrast. However, dividing the contrast range among islands does not necessarily mean that a close contrast estimation will be obtained in the end. In fact, no EA could guaranty it. But the problem itself is not too easy in the sense that there are many local minima which have comparable evaluations with the global optimum. However, it still can be useful as initial solution since it does provide reasonable solutions for contrast and electric field images.
		
		\subsection{Application to Nonlinear Stochastic Methods}\label{chap:results:ca:application}
		
			The application of CA as initialization operator is preliminary analyzed through the following case study: a centered triangle whose side length is $0.3\sqrt{3}\lambda_b$. Its contrast was set to $5$. This configuration is above the curve in \ref{fig:annex:contrastsize:size}. Instead of setting the number of measurements and sources according to the degrees of freedom, they both were set to 16 which is significantly smaller. This choice was made to avoid excessive computational effort to synthesize the scattered field and reduce the number of variables. The data was corrupted with 1\% noise level. The IMSA-L-SHADE algorithm was used for both configurations. The population size was set to 250 and the number of evaluations was set to 50,000 for each resolution step. Different resolution steps were considered for each algorithm: (i) BA addressed $5\times5$, $7\times7$, and $10\times10$; and (ii) CA addressed $10\times10$, $20\times20$, and $30\times30$. CA was run with higher resolutions since the initialization already provides a precise shape. The contrast range was set differently for each algorithm: while the algorithm initialization by BA was allowed to search for contrast values between 0 and 5, the one initialized by CA was allowed to search between 0 and 50. Five islands were used to divide the contrast range in CA and each island was allowed to run 1,000 evaluations.
			
			The best images recovered by each algorithm are shown in \autoref{fig:results:ca:res:images}. Even though the best image recovered by BA suggest that the algorithm could detect a scatterer at the center, the estimated contrast is too low. The best image recovered by CA still looks like a circle but with comparable size and contrast than true profile. Although some pixels within the object area indicate that the optimization algorithm explored solutions nearby the one provided by the initialization method, the ones close to the object are very homogeneous. This suggests that small perturbations around the circle either were not promoted or were not successful. This might be due to a poor diversity among the individuals when the population was initialized. If very different contrast images are not part of the initial population, even that their objective function evaluation are high, then the algorithm will not be able to explore reasonably the search space and it will converge prematurely.
		
			\begin{figure}
				\centering
				\subfloat[Instance]{\includegraphics[width=.3\textwidth]{./figuras/results/ca_res_inst}}
				\subfloat[Born Approximation]{\includegraphics[width=.3\textwidth]{./figuras/results/ca_res_ba}}
				\subfloat[CA]{\includegraphics[width=.3\textwidth]{./figuras/results/ca_res_ca}}
				\caption[Initialization operator: best images recovered by each algorithm.]{Initialization operator: best images recovered by each algorithm among 30 executions according to minimum objective function evaluation criterion.}
				\label{fig:results:ca:res:images}
			\end{figure}
		
			\autoref{fig:results:ca:res:conv} shows the convergence. Although the images recovered by BA were not precise, the algorithm reached a stable convergence in each resolution. This means that the algorithm used to get stuck at a local minimum. The initialization promoted by CA used to provide solution with lower evaluation. However, the algorithm still improved the best solution throughout the first resolution. This, unfortunately, was not observed in the following resolution steps.
		
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth]{./figuras/results/ca_res_conv}
				\caption[Initialization operator: convergence.]{Initialization operator: convergence of objective function by each algorithm considering 30 executions.}
				\label{fig:results:ca:res:conv}
			\end{figure}
		
			As expected, a clear difference in performance between the algorithms is seen through the $\zeta_{\epsilon PAD}$ and $\zeta_{\epsilon OE}$ indicators shown in \autoref{fig:results:ca:res:indicators}. Regarding the $\zeta_{\epsilon OE}$ indicator, two observations are made: (i) the difference in shape between true and recovered profiles is probably the reason the algorithm did not reach lower errors; and (ii) considering that the contrast values were allowed to vary between 0 and 50, the error values suggest that either the best island in the initialization method was always the one which contained the true value or, even though much higher contrasts circles were in the initial population, the algorithm was robust to avoid these false solutions. This last observation may indicate the feasibility of applying the initialization method even with the success rate estimated in the previous section. Regarding $\zeta_{S}$, the best case was around 40\%, i.e., the mismatch area was around 40\% of the true one.
		
			\begin{figure}
				\centering
				\subfloat[$\zeta_{\epsilon PAD}$]{\includegraphics[width=.5\textwidth]{./figuras/results/ca_res_epad}}
				\subfloat[$\zeta_{\epsilon OE}$]{\includegraphics[width=.5\textwidth]{./figuras/results/ca_res_eoe}} \\
				\subfloat[$\zeta_{S}$]{\includegraphics[width=.5\textwidth]{./figuras/results/ca_res_s}} 
				\subfloat[Objective Function]{\includegraphics[width=.5\textwidth]{./figuras/results/ca_res_fx}}
				\caption[Initialization operator: indicators.]{Initialization operator: quartiles of performance indicators.}
				\label{fig:results:ca:res:indicators}
			\end{figure}
		
	\section{Conclusion}\label{chap:results:conclusion}
	
		This chapter has presented the preliminary results of the implementations proposed in Subsections \ref{chap:investigation:proposal:stateofart} and \ref{chap:investigation:proposal:stateofart}. The example of the use of the library described in Subsection \ref{chap:investigation:proposal:performance} was left to Appendix \ref{annex:bimreview}. Case studies were performed to test the algorithms in simple scenarios exploring few properties. Therefore, the results are still not able to answer the questions raised in the aforementioned subsections. However, some considerations are already possible.
		
		Regarding L-SHADE, clear differences in performance were seen when compared against GA and PSO, either in single or multiple resolutions formulations. A more complete experimentation should be considered not only to verify if this difference will be noticed in different situations but also to quantify this difference. However, it is still not clear if L-SHADE will outperform DE. Even though DE seems to converge faster in single resolution formulations, the L-SHADE was able to recover the best images in harder scenarios (high single resolution and multiples). More experiments should definitely be considered since it is very important to answer whether the use of a more advanced technique is justified, since L-SHADE is a self-adaptive implementation of DE. The population reduction strategy embedded into L-SHADE's formulation might not be relevant since no clear difference against SHADE, its version without this strategy, was seen in the considered experiments. In conclusion, L-SHADE can also be considered as promising technique especially due the results obtained with the IMSA strategy.  
		
		In addition, the clear difference in performance between DE and PSO noticed in the results gave rise to the following question: why has PSO received more attention than DE in the last years in the literature? In \citep{rekanos2008shape}, DE has outperformed PSO considering PEC shape reconstructions with synthesized data. In \citep{caorsi2003detection}, the authors proposed applied DE but they did not compared against other methodologies. In \citep{donelli2010differential}, the authors proposed to combine DE and IMSA but they did not compared against IMSA-PSO which had already been proposed before by the same authors \citep{donelli2006integrated}. In \citep{rocca2009evolutionary}, the authors pointed out some advantages that PSO has over DE and GA due its structures. But no reference was given demonstrating numerical results of these advantages, especially regarding the comparison against DE. In \citep{salucci2017multifrequency}, the authors did not discuss the method preference. Although the clear difference was not expected due to recent choices made on the literature, the obtained results do not seem to be a contradiction since it lacks a robust experimentation that can justify any preference. The difference in performance might be associated with a higher diverse exploration that the mutation operator in DE might promote using three random individuals. This strategy may be relevant in large scale and nonlinear problems.
		
		Regarding CA, for single and perfect dielectric scatterers, the initialization seems to be robust to detect location and position. Although the demonstration study suggested some unreliability on detecting the true contrast interval, the experiment with the non-linear method showed that the final solution was always around the true value. No experiment with multiple scatterers was consider yet. Although it is an important test, some modifications should be considered in order to improve that correct estimation of contrast range and promote more diversity from the solution obtained from the initialization method. To improve the correct estimation of the contrast range, the radius and position estimation might be fixed after some generations when all the solutions tended to close positions and radius. Then the optimization problem will become unidimensional and efficient methods may be applied. To improve diversity when generating the initial population from the initialization method, the distortions on the circle, such as random cuts or insertions, might be considered. The electric field images, in turn, can be diversified with the use of the other functions, such as $H^{(2)}_0(x)$. In conclusion, the results encourage to continue exploring the method as a potential one to address strong scatterers.
		
		Lastly, it is worth noting that most of the algorithms implemented in this work depend on the choice of some parameters, such as mutation probability, crossover probability, weights, etc. We acknowledge that this might impact the performance of the algorithms. However, a robust adjustment of these parameters is out of the scope of this investigation. This could be achieved with some methodologies, such as Iterated F-Race \citep{birattari2010frace}. But there must be attention to a possible dependency between adjustment and instance patterns (contrast, size, noise etc).
		
		